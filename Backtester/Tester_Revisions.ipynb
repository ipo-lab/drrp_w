{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = r\"C:\\Users\\Rafay\\Documents\\thesis3\\thesis\\ActualWork\\e2e\\cache\"\n",
    "factors_list = ['RF']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData(path_to_data, e2e=True, datatype='broad'):\n",
    "    if e2e:\n",
    "        path_to_returns = r'{}\\asset_weekly_{}.pkl'.format(path_to_data, datatype)\n",
    "        path_to_prices = r'{}\\assetprices_weekly_{}.pkl'.format(path_to_data, datatype)\n",
    "        path_to_factors = r'{}\\factor_weekly_{}.pkl'.format(path_to_data, datatype)\n",
    "\n",
    "        returns = pd.read_pickle(path_to_returns)\n",
    "        prices = pd.read_pickle(path_to_prices)\n",
    "        factors = pd.read_pickle(path_to_factors)\n",
    "\n",
    "        assets_list = prices.columns.to_list()\n",
    "\n",
    "        returns = returns.reset_index()\n",
    "        prices = prices.reset_index()\n",
    "        factors = factors.reset_index()\n",
    "\n",
    "        factors = factors.rename(columns={\"Date\": \"date\", \"Mkt-RF\": \"RF\"})\n",
    "        factors = factors[['date'] + factors_list]\n",
    "\n",
    "        return returns, assets_list, prices, factors\n",
    "\n",
    "    path_to_prices = r'{}\\prices.csv'.format(path_to_data)\n",
    "    path_to_factors = r'{}\\3factors.csv'.format(path_to_data)\n",
    "\n",
    "    prices = pd.read_csv(path_to_prices)\n",
    "    factors = pd.read_csv(path_to_factors)\n",
    "\n",
    "    assets_list = list(prices['symbol'].unique())\n",
    "\n",
    "    assets_list_cleaned = [x for x in assets_list if str(x) != 'nan']\n",
    "    pivot_prices = np.round(pd.pivot_table(prices, values='close', \n",
    "                                    index='date', \n",
    "                                    columns='symbol', \n",
    "                                    aggfunc=np.mean),2)\n",
    "    pivot_prices = pivot_prices.reset_index()\n",
    "    pivot_prices['date'] = pd.to_datetime(pivot_prices['date'])\n",
    "    factors['date'] = pd.to_datetime(factors['Date'], format=\"%Y%m%d\")\n",
    "\n",
    "    pivot_prices = pivot_prices.set_index('date')\n",
    "    returns = pivot_prices.pct_change()\n",
    "    pivot_prices = pivot_prices.reset_index()\n",
    "    returns = returns.reset_index()\n",
    "    returns = returns.merge(factors, on='date', how='left')\n",
    "    returns = returns.drop(['Date'], axis=1)\n",
    "    returns = returns.dropna()\n",
    "\n",
    "    return returns, assets_list_cleaned, pivot_prices, []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype='cross_asset'\n",
    "returns, assets_list_cleaned, prices, factors = LoadData(path_to_data, e2e=True, datatype=datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>FTGC</th>\n",
       "      <th>BNDX</th>\n",
       "      <th>SPY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>-0.008560</td>\n",
       "      <td>-0.001609</td>\n",
       "      <td>0.003071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-10</td>\n",
       "      <td>0.003557</td>\n",
       "      <td>-0.004206</td>\n",
       "      <td>0.003688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-17</td>\n",
       "      <td>-0.000838</td>\n",
       "      <td>-0.002796</td>\n",
       "      <td>-0.013519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-24</td>\n",
       "      <td>-0.019560</td>\n",
       "      <td>-0.001197</td>\n",
       "      <td>0.034661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-31</td>\n",
       "      <td>-0.032443</td>\n",
       "      <td>-0.005337</td>\n",
       "      <td>0.022047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date      FTGC      BNDX       SPY\n",
       "0 2014-01-03 -0.008560 -0.001609  0.003071\n",
       "1 2014-01-10  0.003557 -0.004206  0.003688\n",
       "2 2014-01-17 -0.000838 -0.002796 -0.013519\n",
       "3 2014-01-24 -0.019560 -0.001197  0.034661\n",
       "4 2014-01-31 -0.032443 -0.005337  0.022047"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def DRRPW(mu,Q, delta=0):\n",
    "    \n",
    "    # # of Assets\n",
    "    n = len(mu)\n",
    "\n",
    "    # Decision Variables\n",
    "    w = cp.Variable(n)\n",
    "\n",
    "    # Kappa\n",
    "    k = 100\n",
    "\n",
    "    # Norm for x\n",
    "    p = 2\n",
    "\n",
    "    constraints = [\n",
    "        w>=0 # Disallow Short Sales\n",
    "    ]\n",
    "\n",
    "    # risk = cp.quad_form(w, Q)\n",
    "\n",
    "    log_term = 0\n",
    "    for i in range(n):\n",
    "        log_term += cp.log(w[i])\n",
    "    \n",
    "    # We need to compute \\sqrt{x^T Q x} intelligently because\n",
    "    # cvxpy does not compute well with the \\sqrt\n",
    "\n",
    "    # To do this, I will take the Cholesky decomposition\n",
    "    # Q = LL^T\n",
    "    # Then, take the 2-norm of L*x\n",
    "\n",
    "    # Idea: (L_1 * x_1)^2 = Q_1 x_1\n",
    "    \n",
    "    L = np.linalg.cholesky(Q)\n",
    "    L /= np.linalg.norm(L)\n",
    "\n",
    "    L = L.T\n",
    "    \n",
    "    obj = cp.power(cp.norm(L@w,2) + math.sqrt(delta)*cp.norm(w, p),2)\n",
    "    obj = obj - k*log_term\n",
    "\n",
    "    prob = cp.Problem(cp.Minimize(obj), constraints=constraints)\n",
    "    \n",
    "    # ECOS fails sometimes, if it does then do SCS\n",
    "    try:\n",
    "        prob.solve(verbose=False)\n",
    "    except:\n",
    "        prob.solve(solver='SCS',verbose=False)\n",
    "    \n",
    "    x = w.value\n",
    "    x = np.divide(x, np.sum(x))\n",
    "    \n",
    "    # Check Risk Parity Condition is actually met\n",
    "    # Note: DRRPW will not meet RP, will meet a robust version of RP\n",
    "    risk_contrib = np.multiply(x, Q.dot(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "class drrpw_custom(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, Y, delta):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        # Compute covariance\n",
    "        Q = np.cov(Y.cpu().detach().numpy(), rowvar=False)\n",
    "\n",
    "        # Get optimal allocation vector\n",
    "        z_star,  = DRRPW(np.ones(Q.shape[0], Q, delta))\n",
    "\n",
    "        # Save vectors\n",
    "        ctx.save_for_backward(Q)\n",
    "        ctx.save_for_backward(delta)\n",
    "        ctx.save_for_backward(z_star)\n",
    "        return z_star\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "drrpw = drrpw_custom()\n",
    "data = torch.from_numpy(asset_returns.to_numpy())\n",
    "drrpw.apply(data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 104\n",
    "date = '2019-06-02'\n",
    "returns_lastn = returns[(returns['date'] < date)].tail(lookback)\n",
    "asset_returns = returns_lastn.drop(['date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>2019-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>2019-07-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>2019-07-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>2019-07-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>2019-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>2021-05-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>2021-06-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>2021-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>2021-06-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>2021-06-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date\n",
       "287 2019-07-05\n",
       "288 2019-07-12\n",
       "289 2019-07-19\n",
       "290 2019-07-26\n",
       "291 2019-08-02\n",
       "..         ...\n",
       "386 2021-05-28\n",
       "387 2021-06-04\n",
       "388 2021-06-11\n",
       "389 2021-06-18\n",
       "390 2021-06-25\n",
       "\n",
       "[104 rows x 1 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287    1562284800000000000\n",
       "288    1562889600000000000\n",
       "289    1563494400000000000\n",
       "290    1564099200000000000\n",
       "291    1564704000000000000\n",
       "              ...         \n",
       "386    1622160000000000000\n",
       "387    1622764800000000000\n",
       "388    1623369600000000000\n",
       "389    1623974400000000000\n",
       "390    1624579200000000000\n",
       "Name: date, Length: 104, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_returns_tensor.size(0) - window_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import math\n",
    "from util import nearestPD, BatchUtils\n",
    "\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import PortfolioClasses as pc\n",
    "import LossFunctions as lf\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drrpw_nominal_learnDelta_batched(n_y):\n",
    "    # Variables\n",
    "    phi = cp.Variable((n_y,1), nonneg=True)\n",
    "    t = cp.Variable()\n",
    "    \n",
    "    # Size of uncertainty set\n",
    "    delta = cp.Parameter(1, nonneg=True)\n",
    "\n",
    "    # L - Square Root of Covariance Matrix\n",
    "    L = cp.Parameter((n_y, n_y))\n",
    "\n",
    "    # Norm for x\n",
    "    p = 2\n",
    "\n",
    "    # Kappa, dont need this to be trainable as the value of this doesnt really matter\n",
    "    k = 100\n",
    "\n",
    "    # Constraints\n",
    "    constraints = [\n",
    "        phi >= 0,\n",
    "        t >= cp.power(cp.norm(L@phi, 2) + delta*cp.norm(phi, p),2)\n",
    "        # t >= cp.power(cp.norm(L@phi, 2) + cp.norm(T@phi, 2),2)\n",
    "    ]\n",
    "\n",
    "    log_term = 0\n",
    "    for i in range(n_y):\n",
    "        log_term += cp.log(phi[i])\n",
    "\n",
    "    obj = (t) - k*log_term\n",
    "\n",
    "    # Objective function\n",
    "    objective = cp.Minimize(obj)    \n",
    "\n",
    "    # Construct optimization problem and differentiable layer\n",
    "    problem = cp.Problem(objective, constraints=constraints)\n",
    "\n",
    "    return CvxpyLayer(problem, parameters=[delta, L], variables=[phi, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gmean\n",
    "\n",
    "def GetParameterEstimates(AssetReturns, FactorReturns, technique='OLS', log=True, bad=False, shrinkage=False):\n",
    "    # Only have OLS implemented so far\n",
    "    if technique!='OLS':\n",
    "        return [], []\n",
    "    \n",
    "    if type(AssetReturns) == pd.core.frame.DataFrame:\n",
    "        AssetReturns_np = AssetReturns.to_numpy()\n",
    "        FactorReturns_np = FactorReturns.to_numpy()\n",
    "    else:\n",
    "        AssetReturns_np = AssetReturns.cpu().detach().numpy()[0]\n",
    "        FactorReturns_np = FactorReturns.cpu().detach().numpy()[0][:-1]\n",
    "\n",
    "    if shrinkage:\n",
    "        Q, average_cor, shrink = GetShrinkageCov(AssetReturns_np)\n",
    "        mu = 1 - (gmean(1+AssetReturns_np))\n",
    "\n",
    "        return mu, Q\n",
    "\n",
    "    if bad:\n",
    "        Q = np.cov(AssetReturns_np, rowvar=False)\n",
    "        mu = 1 - (gmean(1+AssetReturns_np))\n",
    "\n",
    "        return mu, Q\n",
    "\n",
    "    T,n = AssetReturns_np.shape\n",
    "    _, p = FactorReturns_np.shape\n",
    "\n",
    "    # Get Data Matrix - Factors\n",
    "    X = np.zeros((T, p+1))\n",
    "    X[:,:-1] = np.ones((T,1)) # Add ones to first row\n",
    "    X[:,1:] = FactorReturns_np\n",
    "\n",
    "    # Get regression coefficients for Assets\n",
    "    # B = (X^TX)^(-1)X^Ty\n",
    "    B = np.matmul(np.linalg.inv((np.matmul(np.transpose(X), X))), (np.matmul(np.transpose(X), AssetReturns_np)))\n",
    "\n",
    "    # Get alpha and betas\n",
    "    a = np.transpose(B[0,:])\n",
    "    V = B[1:(p+1),:]\n",
    "\n",
    "    # Residual Variance to get D\n",
    "    ep = AssetReturns_np - np.matmul(X, B)\n",
    "    sigma_ep = 1/(T-p-1) * np.sum(np.square(ep), axis=0)\n",
    "    D = np.diag(sigma_ep)\n",
    "\n",
    "    # Get Factor Estimated Return and Covariance Matrix\n",
    "    f_bar = np.transpose(np.mean(FactorReturns_np, axis=0))\n",
    "    F = np.cov(FactorReturns_np, rowvar=False)\n",
    "\n",
    "    # Get mu\n",
    "    mu = a + np.matmul(np.transpose(V), f_bar)\n",
    "\n",
    "    # Get Q\n",
    "    Q = np.matmul(np.matmul(np.transpose(V), F), V) + D\n",
    "\n",
    "    # Make sure Q is PSD\n",
    "    w,v = np.linalg.eig(Q)\n",
    "    min_eig = np.min(w)\n",
    "\n",
    "\n",
    "    if min_eig<0:\n",
    "        print('--Not PSD--Adding Min Eigenvalue--')\n",
    "        Q -= min_eig*np.identity(n)\n",
    "\n",
    "    if log:\n",
    "        print(\"Shape of X: {}\".format(X.shape))\n",
    "        print(\"Shape of B: {}\".format(B.shape))\n",
    "        print(\"Shape of X*B: {}\".format(np.matmul(X, B).shape))\n",
    "        print(\"Shape of ep: {}\".format(ep.shape))\n",
    "        print(\"Shape of sigma_ep: {}\".format(sigma_ep.shape))\n",
    "        print(\"Shape of D: {}\".format(sigma_ep.shape))\n",
    "        print(\"Shape of Q: {}\".format(Q.shape))\n",
    "    \n",
    "    return mu, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RP(mu,L):\n",
    "    \n",
    "    # # of Assets\n",
    "    n = len(mu)\n",
    "\n",
    "    # Decision Variables\n",
    "    w = cp.Variable(n)\n",
    "\n",
    "    # Kappa\n",
    "    k = 2\n",
    "          \n",
    "    constraints = [\n",
    "        w>=0 # Disallow Short Sales\n",
    "    ]\n",
    "\n",
    "    # Objective Function\n",
    "    risk = cp.norm(L@w,2)\n",
    "    log_term = 0\n",
    "    for i in range(n):\n",
    "        log_term += cp.log(w[i])\n",
    "    \n",
    "    prob = cp.Problem(cp.Minimize(risk-(k*log_term)), constraints=constraints)\n",
    "    \n",
    "    # ECOS fails sometimes, if it does then do SCS\n",
    "    try:\n",
    "        prob.solve(verbose=False)\n",
    "    except:\n",
    "        prob.solve(solver='SCS',verbose=False)\n",
    "\n",
    "    x = w.value\n",
    "    x = np.divide(x, np.sum(x))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchUtils:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def convert_to_sw_batched(self, input_numpy, window_size):\n",
    "        # convert numpy array to tensor\n",
    "        input_tensor = torch.from_numpy(input_numpy.to_numpy())\n",
    "\n",
    "        # calculate number of training points\n",
    "        num_training_points = input_tensor.size(0) - window_size + 1\n",
    "\n",
    "        # initialize tensor to hold sliding windows\n",
    "        sliding_windows = torch.zeros((num_training_points, window_size, input_tensor.size(1)))\n",
    "\n",
    "        # populate tensor with sliding windows\n",
    "        for i in range(num_training_points):\n",
    "            sliding_windows[i] = input_tensor[i:i+window_size]\n",
    "        \n",
    "        return sliding_windows\n",
    "\n",
    "    def convert_performance_periods(self, input_numpy, window_size):\n",
    "        input_tensor = torch.from_numpy(input_numpy.to_numpy())\n",
    "        num_training_points = input_tensor.size(0) - window_size + 1\n",
    "\n",
    "        # define label tensor shape\n",
    "        label_shape = (num_training_points, 1, input_tensor.size(1))\n",
    "\n",
    "        # initialize label tensor to zeros\n",
    "        label_set = torch.zeros(label_shape)\n",
    "\n",
    "        # populate label tensor with next time period returns\n",
    "        for i in range(num_training_points-1):\n",
    "            label_set[i:i+1] = input_tensor[i+window_size:i+window_size+1].unsqueeze(1)\n",
    "\n",
    "        return label_set\n",
    "    \n",
    "    def compute_covariance_matrix(self, sliding_windows):\n",
    "        num_batches, window_size, num_assets = sliding_windows.size()\n",
    "\n",
    "        # Compute the covariance matrix for all batches simultaneously\n",
    "        Q = torch.Tensor(torch.cov(sliding_windows.permute(0, 2, 1)))\n",
    "\n",
    "        # Initialize the covariance matrix tensor\n",
    "        covariance_matrix_tensor = torch.zeros((num_batches, num_assets, num_assets))\n",
    "\n",
    "        L = torch.cholesky(Q)\n",
    "        L /= torch.norm(L)\n",
    "\n",
    "        # Repeat the Cholesky matrix for each batch\n",
    "        covariance_matrix_tensor = L.unsqueeze(0).expand(num_batches, -1, -1)\n",
    "\n",
    "        return covariance_matrix_tensor\n",
    "batch_utils = BatchUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =torch.tensor([[-5.5405],\n",
    "        [-5.9145],\n",
    "        [-5.6377]])\n",
    "test2 = torch.tensor([0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = torch.tensor([[0.3333], \n",
    "        [0.3333], \n",
    "        [0.4354]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3333, 0.3333, 0.4354])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma = torch.tensor([[ 0.6721,  0.0000,  0.0000],\n",
    "        [ 0.0155,  0.1084,  0.0000],\n",
    "        [ 0.0084, -0.1868,  0.7081]])\n",
    "phi = torch.tensor([[0.3333],\n",
    "        [0.3333],\n",
    "        [0.3333]])\n",
    "num_assets=3\n",
    "delta=torch.tensor([0.0059], requires_grad=True)\n",
    "k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(phi):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "start, end = '2016-01-01', '2022-12-31'\n",
    "# start, end = '2015-01-01', '2015-01-31'\n",
    "factors_list = ['RF', 'SMB', 'HML']\n",
    "factors_list = ['RF']\n",
    "\n",
    "class BatchUtils:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def convert_to_sw_batched(self, input_numpy, window_size):\n",
    "        # convert numpy array to tensor\n",
    "        input_tensor = torch.from_numpy(input_numpy.to_numpy())\n",
    "\n",
    "        # calculate number of training points\n",
    "        num_training_points = input_tensor.size(0) - window_size + 1\n",
    "\n",
    "        # initialize tensor to hold sliding windows\n",
    "        sliding_windows = torch.zeros((num_training_points, window_size, input_tensor.size(1)))\n",
    "\n",
    "        # populate tensor with sliding windows\n",
    "        for i in range(num_training_points):\n",
    "            sliding_windows[i] = input_tensor[i:i+window_size]\n",
    "        \n",
    "        return sliding_windows\n",
    "\n",
    "    def convert_performance_periods(self, input_numpy, window_size):\n",
    "        input_tensor = torch.from_numpy(input_numpy.to_numpy())\n",
    "        num_training_points = input_tensor.size(0) - window_size + 1\n",
    "\n",
    "        # define label tensor shape\n",
    "        label_shape = (num_training_points, input_tensor.size(1), 1)\n",
    "\n",
    "        # initialize label tensor to zeros\n",
    "        label_set = torch.zeros(label_shape)\n",
    "\n",
    "        # populate label tensor with next time period returns\n",
    "        for i in range(num_training_points-1):\n",
    "            label_set[i:i+1] = input_tensor[i+window_size:i+window_size+1].unsqueeze(2)\n",
    "\n",
    "        return label_set\n",
    "    \n",
    "    def compute_covariance_matrix(self, sliding_windows):\n",
    "        # get the number of batches and number of assets\n",
    "        num_batches, window_size, num_assets = sliding_windows.size()\n",
    "\n",
    "        # initialize covariance matrix tensor\n",
    "        covariance_matrix_tensor = torch.zeros((num_batches, num_assets, num_assets))\n",
    "\n",
    "        # compute covariance matrix for each batch\n",
    "        for i in range(num_batches):\n",
    "            batch = sliding_windows[i]\n",
    "            Q = torch.Tensor(np.cov(batch.numpy().T))\n",
    "            try:\n",
    "                L = np.linalg.cholesky(Q)\n",
    "            except:\n",
    "                Q = nearestPD(Q)\n",
    "                L = np.linalg.cholesky(Q)\n",
    "            L /= np.linalg.norm(L)\n",
    "            covariance_matrix_tensor[i] = torch.from_numpy(L)\n",
    "\n",
    "        return covariance_matrix_tensor\n",
    "\n",
    "def LoadData(path_to_data, e2e=True, datatype='broad'):\n",
    "    if e2e:\n",
    "        path_to_returns = r'{}\\asset_weekly_{}.pkl'.format(path_to_data, datatype)\n",
    "        path_to_prices = r'{}\\assetprices_weekly_{}.pkl'.format(path_to_data, datatype)\n",
    "        path_to_factors = r'{}\\factor_weekly_{}.pkl'.format(path_to_data, datatype)\n",
    "\n",
    "        returns = pd.read_pickle(path_to_returns)\n",
    "        prices = pd.read_pickle(path_to_prices)\n",
    "        factors = pd.read_pickle(path_to_factors)\n",
    "\n",
    "        assets_list = prices.columns.to_list()\n",
    "\n",
    "        returns = returns.reset_index()\n",
    "        prices = prices.reset_index()\n",
    "        factors = factors.reset_index()\n",
    "\n",
    "        factors = factors.rename(columns={\"Date\": \"date\", \"Mkt-RF\": \"RF\"})\n",
    "        factors = factors[['date'] + factors_list]\n",
    "\n",
    "        return returns, assets_list, prices, factors\n",
    "\n",
    "    path_to_prices = r'{}\\prices.csv'.format(path_to_data)\n",
    "    path_to_factors = r'{}\\3factors.csv'.format(path_to_data)\n",
    "\n",
    "    prices = pd.read_csv(path_to_prices)\n",
    "    factors = pd.read_csv(path_to_factors)\n",
    "\n",
    "    assets_list = list(prices['symbol'].unique())\n",
    "\n",
    "    assets_list_cleaned = [x for x in assets_list if str(x) != 'nan']\n",
    "    pivot_prices = np.round(pd.pivot_table(prices, values='close', \n",
    "                                    index='date', \n",
    "                                    columns='symbol', \n",
    "                                    aggfunc=np.mean),2)\n",
    "    pivot_prices = pivot_prices.reset_index()\n",
    "    pivot_prices['date'] = pd.to_datetime(pivot_prices['date'])\n",
    "    factors['date'] = pd.to_datetime(factors['Date'], format=\"%Y%m%d\")\n",
    "\n",
    "    pivot_prices = pivot_prices.set_index('date')\n",
    "    returns = pivot_prices.pct_change()\n",
    "    pivot_prices = pivot_prices.reset_index()\n",
    "    returns = returns.reset_index()\n",
    "    returns = returns.merge(factors, on='date', how='left')\n",
    "    returns = returns.drop(['Date'], axis=1)\n",
    "    returns = returns.dropna()\n",
    "\n",
    "    return returns, assets_list_cleaned, pivot_prices, []\n",
    "\n",
    "def shrinkage(returns):\n",
    "    \"\"\"Shrinks sample covariance matrix towards constant correlation unequal variance matrix.\n",
    "    Ledoit & Wolf (\"Honey, I shrunk the sample covariance matrix\", Portfolio Management, 30(2004),\n",
    "    110-119) optimal asymptotic shrinkage between 0 (sample covariance matrix) and 1 (constant\n",
    "    sample average correlation unequal sample variance matrix).\n",
    "    Paper:\n",
    "    http://www.ledoit.net/honey.pdf\n",
    "    Matlab code:\n",
    "    https://www.econ.uzh.ch/dam/jcr:ffffffff-935a-b0d6-ffff-ffffde5e2d4e/covCor.m.zip\n",
    "    Special thanks to Evgeny Pogrebnyak https://github.com/epogrebnyak\n",
    "    :param returns:\n",
    "        t, n - returns of t observations of n shares.\n",
    "    :return:\n",
    "        Covariance matrix, sample average correlation, shrinkage.\n",
    "    \"\"\"\n",
    "    t, n = returns.shape\n",
    "    mean_returns = np.mean(returns, axis=0, keepdims=True)\n",
    "    returns -= mean_returns\n",
    "    sample_cov = returns.transpose() @ returns / t\n",
    "\n",
    "    # sample average correlation\n",
    "    var = np.diag(sample_cov).reshape(-1, 1)\n",
    "    sqrt_var = var ** 0.5\n",
    "    unit_cor_var = sqrt_var * sqrt_var.transpose()\n",
    "    average_cor = ((sample_cov / unit_cor_var).sum() - n) / n / (n - 1)\n",
    "    prior = average_cor * unit_cor_var\n",
    "    np.fill_diagonal(prior, var)\n",
    "\n",
    "    # pi-hat\n",
    "    y = returns ** 2\n",
    "    phi_mat = (y.transpose() @ y) / t - sample_cov ** 2\n",
    "    phi = phi_mat.sum()\n",
    "\n",
    "    # rho-hat\n",
    "    theta_mat = ((returns ** 3).transpose() @ returns) / t - var * sample_cov\n",
    "    np.fill_diagonal(theta_mat, 0)\n",
    "    rho = (\n",
    "        np.diag(phi_mat).sum()\n",
    "        + average_cor * (1 / sqrt_var @ sqrt_var.transpose() * theta_mat).sum()\n",
    "    )\n",
    "\n",
    "    # gamma-hat\n",
    "    gamma = np.linalg.norm(sample_cov - prior, \"fro\") ** 2\n",
    "\n",
    "    # shrinkage constant\n",
    "    kappa = (phi - rho) / gamma\n",
    "    shrink = max(0, min(1, kappa / t))\n",
    "\n",
    "    # estimator\n",
    "    sigma = shrink * prior + (1 - shrink) * sample_cov\n",
    "\n",
    "    return sigma, average_cor, shrink\n",
    "\n",
    "def generate_date_list(data, data2, start, end):\n",
    "    start = datetime.fromisoformat(start)\n",
    "    end = datetime.fromisoformat(end)\n",
    "\n",
    "    # Must be in this list\n",
    "    must = data2.date.apply(lambda x: x.date()).unique().tolist()\n",
    "\n",
    "    # Train model from start_date to date\n",
    "    mask = (data['date'] >= start) & (data['date'] <= end) & data['date'].isin(must)\n",
    "\n",
    "    data = data.loc[mask]\n",
    "    return data.date.apply(lambda x: x.date()).unique().tolist()\n",
    "\n",
    "def isPD(B):\n",
    "    \"\"\"Returns true when input is positive-definite, via Cholesky\"\"\"\n",
    "    try:\n",
    "        _ = np.linalg.cholesky(B)\n",
    "        return True\n",
    "    except np.linalg.LinAlgError:\n",
    "        return False\n",
    "\n",
    "def nearestPD(A):\n",
    "    \"\"\"Find the nearest positive-definite matrix to input\n",
    "\n",
    "    A Python/Numpy port of John D'Errico's `nearestSPD` MATLAB code [1], which\n",
    "    credits [2].\n",
    "\n",
    "    [1] https://www.mathworks.com/matlabcentral/fileexchange/42885-nearestspd\n",
    "\n",
    "    [2] N.J. Higham, \"Computing a nearest symmetric positive semidefinite\n",
    "    matrix\" (1988): https://doi.org/10.1016/0024-3795(88)90223-6\n",
    "    \"\"\"\n",
    "\n",
    "    B = (A + A.T) / 2\n",
    "    _, s, V = np.linalg.svd(B)\n",
    "\n",
    "    H = np.dot(V.T, np.dot(np.diag(s), V))\n",
    "\n",
    "    A2 = (B + H) / 2\n",
    "\n",
    "    A3 = (A2 + A2.T) / 2\n",
    "\n",
    "    if isPD(A3):\n",
    "        return A3\n",
    "\n",
    "    spacing = np.spacing(np.linalg.norm(A))\n",
    "    # The above is different from [1]. It appears that MATLAB's `chol` Cholesky\n",
    "    # decomposition will accept matrixes with exactly 0-eigenvalue, whereas\n",
    "    # Numpy's will not. So where [1] uses `eps(mineig)` (where `eps` is Matlab\n",
    "    # for `np.spacing`), we use the above definition. CAVEAT: our `spacing`\n",
    "    # will be much larger than [1]'s `eps(mineig)`, since `mineig` is usually on\n",
    "    # the order of 1e-16, and `eps(1e-16)` is on the order of 1e-34, whereas\n",
    "    # `spacing` will, for Gaussian random matrixes of small dimension, be on\n",
    "    # othe order of 1e-16. In practice, both ways converge, as the unit test\n",
    "    # below suggests.\n",
    "    I = np.eye(A.shape[0])\n",
    "    k = 1\n",
    "    while not isPD(A3):\n",
    "        mineig = np.min(np.real(np.linalg.eigvals(A3)))\n",
    "        A3 += I * (-mineig * k**2 + spacing)\n",
    "        k += 1\n",
    "\n",
    "    return A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([ 802.8609,  706.1788,  742.6114, -472.8843])[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DRRPW(mu,Q, delta=0):\n",
    "    \n",
    "    # # of Assets\n",
    "    n = len(mu)\n",
    "\n",
    "    # Decision Variables\n",
    "    w = cp.Variable(n)\n",
    "\n",
    "    # Kappa\n",
    "    k = 100\n",
    "\n",
    "    # Norm for x\n",
    "    p = 2\n",
    "\n",
    "    constraints = [\n",
    "        w>=0 # Disallow Short Sales\n",
    "    ]\n",
    "\n",
    "    # risk = cp.quad_form(w, Q)\n",
    "\n",
    "    log_term = 0\n",
    "    for i in range(n):\n",
    "        log_term += cp.log(w[i])\n",
    "    \n",
    "    # We need to compute \\sqrt{x^T Q x} intelligently because\n",
    "    # cvxpy does not compute well with the \\sqrt\n",
    "\n",
    "    # To do this, I will take the Cholesky decomposition\n",
    "    # Q = LL^T\n",
    "    # Then, take the 2-norm of L*x\n",
    "\n",
    "    # Idea: (L_1 * x_1)^2 = Q_1 x_1\n",
    "    # Q = nearestPD(Q)\n",
    "    L = Q.T\n",
    "\n",
    "    obj = cp.power(cp.norm(L@w,2) + math.sqrt(delta)*cp.norm(w, p),2)\n",
    "    obj = obj - k*log_term\n",
    "\n",
    "    prob = cp.Problem(cp.Minimize(obj), constraints=constraints)\n",
    "    \n",
    "    # ECOS fails sometimes, if it does then do SCS\n",
    "    try:\n",
    "        prob.solve(verbose=False)\n",
    "    except:\n",
    "        prob.solve(solver='SCS',verbose=False)\n",
    "    \n",
    "    x = w.value\n",
    "    x = np.divide(x, np.sum(x))\n",
    "    \n",
    "    # Check Risk Parity Condition is actually met\n",
    "    # Note: DRRPW will not meet RP, will meet a robust version of RP\n",
    "    risk_contrib = np.multiply(x, Q.dot(x))\n",
    "\n",
    "    return x\n",
    "def RP(mu,Q):\n",
    "    \n",
    "    # # of Assets\n",
    "    n = len(mu)\n",
    "\n",
    "    # Decision Variables\n",
    "    w = cp.Variable(n)\n",
    "\n",
    "    # Kappa\n",
    "    k = 2\n",
    "          \n",
    "    constraints = [\n",
    "        w>=0 # Disallow Short Sales\n",
    "    ]\n",
    "\n",
    "    L = Q\n",
    "\n",
    "    # L = np.linalg.cholesky(Q)\n",
    "    # L /= np.linalg.norm(L)\n",
    "\n",
    "    # L = L.T\n",
    "\n",
    "    # Objective Function\n",
    "    risk = cp.norm(L@w,2)\n",
    "    log_term = 0\n",
    "    for i in range(n):\n",
    "        log_term += cp.log(w[i])\n",
    "    \n",
    "    prob = cp.Problem(cp.Minimize(risk-(k*log_term)), constraints=constraints)\n",
    "    \n",
    "    # ECOS fails sometimes, if it does then do SCS\n",
    "    try:\n",
    "        prob.solve(verbose=False)\n",
    "    except:\n",
    "        prob.solve(solver='SCS',verbose=False)\n",
    "\n",
    "    x = w.value\n",
    "    x = np.divide(x, np.sum(x))\n",
    "\n",
    "    # Check Risk Parity Condition is actually met\n",
    "    risk_contrib = np.multiply(x, Q.dot(x))\n",
    "    if not np.all(np.isclose(risk_contrib, risk_contrib[0])):\n",
    "        print(\"RP did not work\")\n",
    "\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import gmean\n",
    "import cvxopt as opt\n",
    "from cvxopt import matrix, spmatrix, sparse\n",
    "from cvxopt.solvers import qp, options\n",
    "from cvxopt import blas\n",
    "import pandas as pd\n",
    "options['show_progress'] = False\n",
    "options['feastol'] = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones((3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class drrpw_newton_util:\n",
    "    def __init__(self):\n",
    "        self.tol = 1e-5\n",
    "        self.max_it = 5000\n",
    "        self.tol_barrier = 1e-5\n",
    "\n",
    "    def f(self,vars, Sigma, num_assets, delta, k):\n",
    "        phi = vars[:num_assets]\n",
    "        lmda = vars[-1]\n",
    "\n",
    "        return ((phi.T @ Sigma @ phi)**0.5 + delta*np.linalg.norm(phi))**2 - k*np.sum(torch.log(phi)) + lmda*(torch.ones(num_assets) @ phi - 1)\n",
    "    \n",
    "    def gradient(self,vars, Sigma, num_assets, delta, k):\n",
    "        phi = vars[:num_assets]\n",
    "        lmda = vars[-1]\n",
    "        \n",
    "        Sigma = Sigma.to(torch.float64)\n",
    "        phi = phi.to(torch.float64)\n",
    "\n",
    "        risk = phi.T @ Sigma @ phi\n",
    "        norm = np.linalg.norm(phi)\n",
    "        grad_phi = 2*(Sigma@phi + delta*risk/norm*phi + delta*norm/risk*(Sigma @ phi) + (delta**2)*phi) - k/phi\n",
    "\n",
    "        # grad_phi = Sigma@phi - k/phi\n",
    "        return grad_phi\n",
    "    \n",
    "    def hessian(self, vars, Sigma, num_assets, delta, k):\n",
    "        phi = vars[:num_assets]\n",
    "        Sigma = Sigma.to(torch.float64)\n",
    "        phi = phi.to(torch.float64)\n",
    "\n",
    "        norm = np.linalg.norm(phi)\n",
    "        risk = phi.T @ Sigma @ phi\n",
    "        sigma_phi = Sigma @ phi\n",
    "\n",
    "        B = 1/(norm*((risk)**0.5))*(sigma_phi @ phi.T) - norm/((risk)**1.5)*(sigma_phi @ (sigma_phi).T) + norm/((risk)**0.5)*Sigma\n",
    "        A_mat = (phi @ (sigma_phi).T)/(norm*((risk)**0.5)) + ((risk)**0.5)/norm*torch.eye(num_assets,num_assets) - ((risk)**0.5)/(norm**3) * (phi @ phi.T)\n",
    "\n",
    "        H = 2*Sigma + 2*(delta*A_mat) + 2*(delta * B) + k*torch.diag(1 / (phi.squeeze()**2)) + 2*(delta ** 2)*torch.eye(num_assets,num_assets)\n",
    "        # H = Sigma + k*torch.diag(1 / (phi.squeeze()**2))\n",
    "        print('-- Hessian --')\n",
    "        print(H)\n",
    "        return H\n",
    "        \n",
    "util = drrpw_newton_util()\n",
    "Sigma = torch.tensor([[ 0.7359, -0.0225,  0.1200],\n",
    "        [-0.0225,  0.1269,  0.0244],\n",
    "        [ 0.1200,  0.0244,  0.6167]])\n",
    "phi = torch.tensor([[0.3333],\n",
    "        [0.3333],\n",
    "        [0.3333], [1.]])\n",
    "\n",
    "phi = phi[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1497394.2259473808"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33156988 0.33603059 0.33239953]\n",
      "RP did not work\n",
      "[0.12529569 0.75387158 0.12083273]\n",
      "Iteration: 0. phi: tensor([[0.3333],\n",
      "        [0.3333],\n",
      "        [0.3333]])\n",
      "-- Hessian --\n",
      "tensor([[4.5178e+06, 3.2311e+02, 6.0319e+02],\n",
      "        [3.2311e+02, 4.5150e+06, 2.7961e+02],\n",
      "        [6.0319e+02, 2.7961e+02, 4.5172e+06]], dtype=torch.float64)\n",
      "Iteration 1. g: tensor([[1498420.9661],\n",
      "        [1495659.9150],\n",
      "        [1497900.5798]], dtype=torch.float64). M: tensor([[ 2.2135e-07, -1.5839e-11, -2.9556e-11],\n",
      "        [-1.5839e-11,  2.2149e-07, -1.3708e-11],\n",
      "        [-2.9556e-11, -1.3708e-11,  2.2138e-07]], dtype=torch.float64). M@g: tensor([[0.3316],\n",
      "        [0.3312],\n",
      "        [0.3315]], dtype=torch.float64). phi: tensor([[0.0017],\n",
      "        [0.0021],\n",
      "        [0.0018]], dtype=torch.float64)\n",
      "-- Hessian --\n",
      "tensor([[5.2706e+08, 4.4445e+02, 5.5185e+02],\n",
      "        [4.4445e+02, 3.5279e+08, 3.9638e+02],\n",
      "        [5.5185e+02, 3.9638e+02, 4.8544e+08]], dtype=torch.float64)\n",
      "Iteration 2. g: tensor([[-875038.6699],\n",
      "        [-713605.9685],\n",
      "        [-839190.4035]], dtype=torch.float64). M: tensor([[ 1.8973e-09, -2.3903e-15, -2.1569e-15],\n",
      "        [-2.3903e-15,  2.8346e-09, -2.3145e-15],\n",
      "        [-2.1569e-15, -2.3145e-15,  2.0600e-09]], dtype=torch.float64). M@g: tensor([[-0.0017],\n",
      "        [-0.0020],\n",
      "        [-0.0017]], dtype=torch.float64). phi: tensor([[0.0034],\n",
      "        [0.0041],\n",
      "        [0.0035]], dtype=torch.float64)\n",
      "-- Hessian --\n",
      "tensor([[1.3785e+08, 4.4280e+02, 5.5272e+02],\n",
      "        [4.4280e+02, 9.3849e+07, 3.9468e+02],\n",
      "        [5.5272e+02, 3.9468e+02, 1.2736e+08]], dtype=torch.float64)\n",
      "Iteration 3. g: tensor([[-429317.5996],\n",
      "        [-347680.3735],\n",
      "        [-411203.0150]], dtype=torch.float64). M: tensor([[ 7.2542e-09, -3.4227e-14, -3.1482e-14],\n",
      "        [-3.4227e-14,  1.0655e-08, -3.3020e-14],\n",
      "        [-3.1482e-14, -3.3020e-14,  7.8517e-09]], dtype=torch.float64). M@g: tensor([[-0.0031],\n",
      "        [-0.0037],\n",
      "        [-0.0032]], dtype=torch.float64). phi: tensor([[0.0065],\n",
      "        [0.0078],\n",
      "        [0.0067]], dtype=torch.float64)\n",
      "-- Hessian --\n",
      "tensor([[4.0368e+07, 4.3478e+02, 5.5684e+02],\n",
      "        [4.3478e+02, 2.9151e+07, 3.8648e+02],\n",
      "        [5.5684e+02, 3.8648e+02, 3.7703e+07]], dtype=torch.float64)\n",
      "Iteration 4. g: tensor([[-199944.4026],\n",
      "        [-157151.9959],\n",
      "        [-190490.8137]], dtype=torch.float64). M: tensor([[ 2.4772e-08, -3.6947e-13, -3.6586e-13],\n",
      "        [-3.6947e-13,  3.4304e-08, -3.5164e-13],\n",
      "        [-3.6586e-13, -3.5164e-13,  2.6523e-08]], dtype=torch.float64). M@g: tensor([[-0.0050],\n",
      "        [-0.0054],\n",
      "        [-0.0051]], dtype=torch.float64). phi: tensor([[0.0114],\n",
      "        [0.0132],\n",
      "        [0.0118]], dtype=torch.float64)\n",
      "-- Hessian --\n",
      "tensor([[1.6009e+07, 4.0782e+02, 5.6991e+02],\n",
      "        [4.0782e+02, 1.3123e+07, 3.5941e+02],\n",
      "        [5.6991e+02, 3.5941e+02, 1.5327e+07]], dtype=torch.float64)\n",
      "Iteration 5. g: tensor([[-77084.1283],\n",
      "        [-54308.0628],\n",
      "        [-72028.2505]], dtype=torch.float64). M: tensor([[ 6.2466e-08, -1.9411e-12, -2.3226e-12],\n",
      "        [-1.9411e-12,  7.6199e-08, -1.7868e-12],\n",
      "        [-2.3226e-12, -1.7868e-12,  6.5244e-08]], dtype=torch.float64). M@g: tensor([[-0.0048],\n",
      "        [-0.0041],\n",
      "        [-0.0047]], dtype=torch.float64). phi: tensor([[0.0162],\n",
      "        [0.0173],\n",
      "        [0.0165]], dtype=torch.float64)\n",
      "-- Hessian --\n",
      "tensor([[1.0197e+07, 3.6037e+02, 5.8998e+02],\n",
      "        [3.6037e+02, 9.4980e+06, 3.1367e+02],\n",
      "        [5.8998e+02, 3.1367e+02, 1.0033e+07]], dtype=torch.float64)\n",
      "Iteration 6. g: tensor([[-16506.7615],\n",
      "        [ -8535.2908],\n",
      "        [-14602.7647]], dtype=torch.float64). M: tensor([[ 9.8064e-08, -3.7205e-12, -5.7662e-12],\n",
      "        [-3.7205e-12,  1.0529e-07, -3.2913e-12],\n",
      "        [-5.7662e-12, -3.2913e-12,  9.9667e-08]], dtype=torch.float64). M@g: tensor([[-0.0016],\n",
      "        [-0.0009],\n",
      "        [-0.0015]], dtype=torch.float64). phi: tensor([[0.0179],\n",
      "        [0.0182],\n",
      "        [0.0179]], dtype=torch.float64)\n",
      "-- Hessian --\n",
      "tensor([[9.2121e+06, 3.3490e+02, 5.9920e+02],\n",
      "        [3.3490e+02, 9.0176e+06, 2.9031e+02],\n",
      "        [5.9920e+02, 2.9031e+02, 9.1723e+06]], dtype=torch.float64)\n",
      "Iteration 7. g: tensor([[-871.2416],\n",
      "        [-227.9370],\n",
      "        [-699.2532]], dtype=torch.float64). M: tensor([[ 1.0855e-07, -4.0312e-12, -7.0914e-12],\n",
      "        [-4.0312e-12,  1.1089e-07, -3.5096e-12],\n",
      "        [-7.0914e-12, -3.5096e-12,  1.0902e-07]], dtype=torch.float64). M@g: tensor([[-9.4570e-05],\n",
      "        [-2.5271e-05],\n",
      "        [-7.6228e-05]], dtype=torch.float64). phi: tensor([[0.0180],\n",
      "        [0.0183],\n",
      "        [0.0180]], dtype=torch.float64)\n",
      "-- Hessian --\n",
      "tensor([[9.1626e+06, 3.3273e+02, 5.9993e+02],\n",
      "        [3.3273e+02, 9.0051e+06, 2.8837e+02],\n",
      "        [5.9993e+02, 2.8837e+02, 9.1328e+06]], dtype=torch.float64)\n",
      "Iteration 8. g: tensor([[-5.0427],\n",
      "        [-0.6268],\n",
      "        [-5.2594]], dtype=torch.float64). M: tensor([[ 1.0914e-07, -4.0323e-12, -7.1692e-12],\n",
      "        [-4.0323e-12,  1.1105e-07, -3.5061e-12],\n",
      "        [-7.1692e-12, -3.5061e-12,  1.0949e-07]], dtype=torch.float64). M@g: tensor([[-5.5032e-07],\n",
      "        [-6.9565e-08],\n",
      "        [-5.7584e-07]], dtype=torch.float64). phi: tensor([[0.0180],\n",
      "        [0.0183],\n",
      "        [0.0180]], dtype=torch.float64)\n",
      "-- Hessian --\n",
      "tensor([[9.1623e+06, 3.3271e+02, 5.9994e+02],\n",
      "        [3.3271e+02, 9.0051e+06, 2.8835e+02],\n",
      "        [5.9994e+02, 2.8835e+02, 9.1326e+06]], dtype=torch.float64)\n",
      "Iteration 9. g: tensor([[-0.0261],\n",
      "        [-0.0028],\n",
      "        [-0.0234]], dtype=torch.float64). M: tensor([[ 1.0914e-07, -4.0323e-12, -7.1697e-12],\n",
      "        [-4.0323e-12,  1.1105e-07, -3.5060e-12],\n",
      "        [-7.1697e-12, -3.5060e-12,  1.0950e-07]], dtype=torch.float64). M@g: tensor([[-2.8452e-09],\n",
      "        [-3.1571e-10],\n",
      "        [-2.5599e-09]], dtype=torch.float64). phi: tensor([[0.0180],\n",
      "        [0.0183],\n",
      "        [0.0180]], dtype=torch.float64)\n",
      "-- Hessian --\n",
      "tensor([[9.1623e+06, 3.3271e+02, 5.9994e+02],\n",
      "        [3.3271e+02, 9.0051e+06, 2.8835e+02],\n",
      "        [5.9994e+02, 2.8835e+02, 9.1326e+06]], dtype=torch.float64)\n",
      "Iteration 10. g: tensor([[-1.1458e-04],\n",
      "        [-1.5336e-05],\n",
      "        [-1.2791e-04]], dtype=torch.float64). M: tensor([[ 1.0914e-07, -4.0323e-12, -7.1697e-12],\n",
      "        [-4.0323e-12,  1.1105e-07, -3.5060e-12],\n",
      "        [-7.1697e-12, -3.5060e-12,  1.0950e-07]], dtype=torch.float64). M@g: tensor([[-1.2504e-11],\n",
      "        [-1.7022e-12],\n",
      "        [-1.4006e-11]], dtype=torch.float64). phi: tensor([[0.0180],\n",
      "        [0.0183],\n",
      "        [0.0180]], dtype=torch.float64)\n",
      "-- Hessian --\n",
      "tensor([[9.1623e+06, 3.3271e+02, 5.9994e+02],\n",
      "        [3.3271e+02, 9.0051e+06, 2.8835e+02],\n",
      "        [5.9994e+02, 2.8835e+02, 9.1326e+06]], dtype=torch.float64)\n",
      "Iteration 11. g: tensor([[-6.3318e-07],\n",
      "        [-6.3126e-08],\n",
      "        [-5.1169e-07]], dtype=torch.float64). M: tensor([[ 1.0914e-07, -4.0323e-12, -7.1697e-12],\n",
      "        [-4.0323e-12,  1.1105e-07, -3.5060e-12],\n",
      "        [-7.1697e-12, -3.5060e-12,  1.0950e-07]], dtype=torch.float64). M@g: tensor([[-6.9103e-14],\n",
      "        [-7.0057e-15],\n",
      "        [-5.6024e-14]], dtype=torch.float64). phi: tensor([[0.0180],\n",
      "        [0.0183],\n",
      "        [0.0180]], dtype=torch.float64)\n",
      "Final phi: tensor([[0.3311],\n",
      "        [0.3367],\n",
      "        [0.3322]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "num_assets = 3\n",
    "alpha = 0.5\n",
    "beta = 0.9\n",
    "delta = 1500\n",
    "\n",
    "print(DRRPW(np.ones(num_assets), Sigma.detach().cpu().numpy(), delta=delta))\n",
    "print(RP(np.ones(num_assets), Sigma.detach().cpu().numpy()))\n",
    "\n",
    "# Newton's method to get grad(phi) = 0.\n",
    "delta = (delta)\n",
    "k=1+delta\n",
    "\n",
    "for it in range(util.max_it):\n",
    "    if it>0:\n",
    "        print(\"Iteration {}. g: {}. M: {}. M@g: {}. phi: {}\".format(it, g, M, M@g, phi))\n",
    "        if torch.linalg.norm(g) <= util.tol:\n",
    "            break\n",
    "    \n",
    "        if torch.any(torch.isnan(phi)):\n",
    "            break\n",
    "    else:\n",
    "        print(\"Iteration: {}. phi: {}\".format(it, phi))\n",
    "    \n",
    "    # Source: Boyd Convex Optimization\n",
    "    # Video: https://www.youtube.com/watch?v=2lPT8j4D1x0\n",
    "    g = util.gradient(phi, Sigma@Sigma.T, num_assets, delta, k)\n",
    "    H = util.hessian(phi, Sigma@Sigma.T, num_assets, delta, k)\n",
    "\n",
    "    M = torch.linalg.inv(H) # TODO replace this with block inverse formulae\n",
    "\n",
    "    # Add h(x) to bottom of gradient tensor\n",
    "    g = g.to(torch.float64)\n",
    "    Sigma = Sigma.to(torch.float64)\n",
    "    phi = phi.to(torch.float64)\n",
    "    # g = torch.cat((g, (torch.ones(num_assets).to(torch.float64)@phi[:num_assets] - 1 + 2*phi[-1].item()).unsqueeze(1)), dim=0)\n",
    "    phi -= (M@g)\n",
    "\n",
    "print(\"Final phi: {}\".format(phi[:3]/sum(phi[:3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0960],\n",
       "        [0.7405],\n",
       "        [0.1634]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[ 1.9552],\n",
    "        [15.0767],\n",
    "        [ 3.3269]])\n",
    "t/torch.sum(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class drrpw_newton_util:\n",
    "    def __init__(self):\n",
    "        self.tol = 1e-5\n",
    "        self.max_it = 10\n",
    "        self.tol_barrier = 1e-5\n",
    "\n",
    "    def f(self,vars, Sigma, num_assets, delta, k):\n",
    "        phi = vars[:num_assets]\n",
    "        lmda = vars[-1]\n",
    "\n",
    "        return ((phi.T @ Sigma @ phi)**0.5 + delta*np.linalg.norm(phi))**2 - k*np.sum(torch.log(phi)) + lmda*(torch.ones(num_assets) @ phi - 1)\n",
    "    \n",
    "    def gradient(self,vars, Sigma, num_assets, delta, k):\n",
    "        phi = vars[:num_assets]\n",
    "        lmda = vars[-1]\n",
    "        \n",
    "        Sigma = Sigma.to(torch.float64)\n",
    "        phi = phi.to(torch.float64)\n",
    "\n",
    "        risk = phi.T @ Sigma @ phi\n",
    "        norm = np.linalg.norm(phi)\n",
    "        grad_phi = 2*(Sigma@phi + delta*risk/norm*phi + delta*norm/risk*(Sigma @ phi) + (delta**2)*phi) - k/phi + lmda.item()*torch.ones((num_assets, 1))\n",
    "        return grad_phi\n",
    "    \n",
    "    def hessian(self, vars, Sigma, num_assets, delta, k):\n",
    "        phi = vars[:num_assets]\n",
    "        Sigma = Sigma.to(torch.float64)\n",
    "        phi = phi.to(torch.float64)\n",
    "\n",
    "        norm = np.linalg.norm(phi)\n",
    "        risk = phi.T @ Sigma @ phi\n",
    "        sigma_phi = Sigma @ phi\n",
    "\n",
    "        B = 1/(norm*((risk)**0.5))*(sigma_phi @ phi.T) - norm/((risk)**1.5)*(sigma_phi @ (sigma_phi).T) + norm/((risk)**0.5)*Sigma\n",
    "        A_mat = (phi @ (sigma_phi).T)/(norm*((risk)**0.5)) + ((risk)**0.5)/norm*torch.eye(num_assets,num_assets) - ((risk)**0.5)/(norm**3) * (phi @ phi.T)\n",
    "\n",
    "        H = 2*Sigma + 2*(delta*A_mat) + 2*(delta * B) + k*torch.diag(1 / (phi.squeeze()**2)) + 2*(delta ** 2)*torch.eye(num_assets,num_assets)\n",
    "        \n",
    "        return H\n",
    "            for it in range(util.max_it):\n",
    "                if it>0:\n",
    "                    print(\"Iteration {}. g: {}. M: {}. M@g: {}. phi: {}\".format(it, g, M, M@g, phi))\n",
    "                else:\n",
    "                    print(\"Iteration: {}. phi: {}\".format(it, phi))\n",
    "                \n",
    "                if torch.linalg.norm(g) <= util.tol:\n",
    "                    break\n",
    "                \n",
    "                if torch.any(torch.isnan(phi)):\n",
    "                    break\n",
    "\n",
    "                # Source: Boyd Convex Optimization\n",
    "                # Video: https://www.youtube.com/watch?v=2lPT8j4D1x0\n",
    "                g = util.gradient(phi, Sigma@Sigma.T, num_assets, delta, k)\n",
    "                H = util.hessian(phi, Sigma@Sigma.T, num_assets, delta, k)\n",
    "\n",
    "                ones = torch.ones(num_assets, 1)  # Column vector of ones\n",
    "\n",
    "                ones = torch.ones(num_assets, 1)  # Column vector of ones\n",
    "                zero = torch.tensor([0.])  # Scalar zero\n",
    "\n",
    "                # Concatenate the blocks to form the block matrix M\n",
    "                top_block = torch.cat((H, ones), dim=1)\n",
    "                bottom_block = torch.cat((ones, zero.unsqueeze(1)), dim=0).t()\n",
    "                H_constrained = torch.cat((top_block, bottom_block), dim=0)\n",
    "                M = torch.linalg.inv(H_constrained) # TODO replace this with block inverse formulae\n",
    "\n",
    "                # Add h(x) to bottom of gradient tensor\n",
    "                g = g.to(torch.float64)\n",
    "                Sigma = Sigma.to(torch.float64)\n",
    "                phi = phi.to(torch.float64)\n",
    "                g = torch.cat((g, (torch.ones(num_assets).to(torch.float64)@phi[:num_assets] - 1).unsqueeze(1)), dim=0)\n",
    "                phi -= (M@g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.4506e-09],\n",
       "        [ 5.7075e-04],\n",
       "        [-5.7074e-04],\n",
       "        [-5.7075e+00]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for it in range(1000):\n",
    "    if torch.linalg.norm(g) <= 1e-6:\n",
    "        break\n",
    "\n",
    "    if it>0:\n",
    "        print(\"New newton step: {}. Current phi: {}. M: {}. grad: {}. M@grad: {}\".format(it, phi, M, g, M@g))\n",
    "    \n",
    "    if torch.any(torch.isnan(phi)):\n",
    "        break\n",
    "\n",
    "    g = gradient(phi)\n",
    "    H = hessian(phi)\n",
    "\n",
    "    M = torch.linalg.inv(H)\n",
    "\n",
    "    phi -= (beta)*(M@g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.tensor([[-5.7075],\n",
    "        [-5.7075],\n",
    "        [-5.7075],\n",
    "        [ 0.0000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tensor([[ 0.0341, -0.0185, -0.0156,  0.3087],\n",
    "        [-0.0182,  0.0367, -0.0186,  0.3679],\n",
    "        [-0.0159, -0.0182,  0.0342,  0.3234],\n",
    "        [ 0.3079,  0.3754,  0.3167, -6.2706]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = phi.T @ Sigma @ phi\n",
    "norm = np.linalg.norm(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do list\n",
    "'''\n",
    "    1. Re-do T and T Diagonal\n",
    "    2. Re-do MVO Norm Learning\n",
    "    3. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.array([[ 4.30433092e+00, -8.36058512e-04, -1.33765299e-02],\n",
    " [ 2.78223877e+00,  2.09977763e-01, -9.87433303e-01],\n",
    " [ 3.17641391e+00, -6.57556446e-01,  1.22038769e+00]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.477716704732"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3008,  1.1426,  0.0753,  1.0837, -0.3898],\n",
      "        [-0.6324,  0.7153, -0.5822,  0.4831,  1.1137],\n",
      "        [ 0.9963, -1.9636, -0.2414, -1.0335, -0.3084],\n",
      "        [ 0.0211, -0.9552,  0.9934,  2.3207, -0.4519],\n",
      "        [-0.1446,  0.2275,  0.7958,  0.1948, -0.0314]])\n",
      "torch.Size([5, 5])\n",
      "tensor([[ 0.3008,  1.1426,  0.0753,  1.0837, -0.3898],\n",
      "        [-0.6324,  0.7153, -0.5822,  0.4831,  1.1137],\n",
      "        [ 0.9963, -1.9636, -0.2414, -1.0335, -0.3084],\n",
      "        [ 0.0211, -0.9552,  0.9934,  2.3207, -0.4519],\n",
      "        [-0.1446,  0.2275,  0.7958,  0.1948, -0.0314]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((5,5))\n",
    "print(x)\n",
    "print(x.size())\n",
    "x = x.repeat(53,1,1)\n",
    "print(x[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2021-01-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cov(): expected input to have two or fewer dimensions but got an input with 3 dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17808\\1474680617.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mbatched_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_sw_batched\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masset_returns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mperformance_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_performance_periods\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masset_returns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mQ_batched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_covariance_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatched_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17808\\742423602.py\u001b[0m in \u001b[0;36mcompute_covariance_matrix\u001b[1;34m(self, sliding_windows)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# Compute the covariance matrix for all batches simultaneously\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msliding_windows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# Initialize the covariance matrix tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cov(): expected input to have two or fewer dimensions but got an input with 3 dimensions"
     ]
    }
   ],
   "source": [
    "lookback = 104\n",
    "returns_lastn = returns[(returns['date'] < date)].tail(lookback)\n",
    "print()\n",
    "asset_returns = returns_lastn.drop(['date'], axis=1)\n",
    "window_size = 52\n",
    "batched_tensor = batch_utils.convert_to_sw_batched(asset_returns, window_size)\n",
    "performance_tensor = batch_utils.convert_performance_periods(asset_returns, window_size)\n",
    "Q_batched = batch_utils.compute_covariance_matrix(batched_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19968\\1208153336.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_pts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtraining\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtesting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "test = [0,1,2,3,4,5,6]\n",
    "window_size = 3\n",
    "training_pts = len(test) - window_size + 1\n",
    "\n",
    "training = []\n",
    "for i in range(training_pts):\n",
    "    training[i] = test[i:i+window_size]\n",
    "\n",
    "testing = []\n",
    "for i in range(training_pts-1):\n",
    "    testing[i:i+1] = test[i+window_size:i+window_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0400, 0.0141, 0.0692]])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_tensor[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0400, 0.0141, 0.0692])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_tensor[10][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 52\n",
    "returns_lastn = returns[(returns['date'] < date)].tail(lookback)\n",
    "asset_returns = returns_lastn.drop(['date'], axis=1)\n",
    "mu, Q = GetParameterEstimates(asset_returns, asset_returns, log=False, bad=True, shrinkage=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5082, 0.0000, 0.0000],\n",
       "        [0.0501, 0.1272, 0.0000],\n",
       "        [0.6650, 0.1414, 0.5107]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_batched[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50819596, 0.        , 0.        ],\n",
       "       [0.05012327, 0.12720189, 0.        ],\n",
       "       [0.66497918, 0.14144394, 0.51072548]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = np.linalg.cholesky(Q)\n",
    "L /= np.linalg.norm(L)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 3])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_batched = torch.tensor(Q)\n",
    "Q_batched.unsqueeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP did not work\n"
     ]
    }
   ],
   "source": [
    "# DRRPW\n",
    "z_drrpw = DRRPW(mu, Q)\n",
    "\n",
    "# Learn Delta\n",
    "L = np.linalg.cholesky(Q)\n",
    "L /= np.linalg.norm(L)\n",
    "Q_batched = torch.tensor(L)\n",
    "Q_batched = Q_batched.unsqueeze(0)\n",
    "Q_batched = Q_batched.type(torch.FloatTensor)\n",
    "batch_size, num_assets, num_assets = Q_batched.size()\n",
    "opt_layer = drrpw_nominal_learnDelta_batched(num_assets)\n",
    "batched_delta = torch.ones((batch_size,1))*0\n",
    "batched_delta = batched_delta.type(torch.FloatTensor)\n",
    "z_stars, _ = opt_layer(batched_delta, Q_batched)\n",
    "\n",
    "# Make each portfolio sum to 1\n",
    "sums = z_stars.sum(dim=1, keepdim=True)\n",
    "z_stars = z_stars / sums\n",
    "\n",
    "# RP\n",
    "z_rp = RP(mu, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRRPW: [0.14172789 0.63553458 0.22273754]\n",
      "Learn Delta: tensor([[[0.1417],\n",
      "         [0.6355],\n",
      "         [0.2227]]])\n",
      "RP: [0.14172835 0.63553333 0.22273832]\n"
     ]
    }
   ],
   "source": [
    "print(\"DRRPW: {}\".format(z_drrpw))\n",
    "print(\"Learn Delta: {}\".format(z_stars))\n",
    "print(\"RP: {}\".format(z_rp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RP(mu,Q):\n",
    "    \n",
    "    # # of Assets\n",
    "    n = len(mu)\n",
    "\n",
    "    # Decision Variables\n",
    "    w = cp.Variable(n)\n",
    "\n",
    "    # Kappa\n",
    "    k = 2\n",
    "          \n",
    "    constraints = [\n",
    "        w>=0 # Disallow Short Sales\n",
    "    ]\n",
    "\n",
    "    L = np.linalg.cholesky(Q)\n",
    "    L /= np.linalg.norm(L)\n",
    "\n",
    "    # Objective Function\n",
    "    risk = cp.norm(L@w,2)\n",
    "    log_term = 0\n",
    "    for i in range(n):\n",
    "        log_term += cp.log(w[i])\n",
    "    \n",
    "    prob = cp.Problem(cp.Minimize(risk-(k*log_term)), constraints=constraints)\n",
    "    \n",
    "    # ECOS fails sometimes, if it does then do SCS\n",
    "    try:\n",
    "        prob.solve(verbose=False)\n",
    "    except:\n",
    "        prob.solve(solver='SCS',verbose=False)\n",
    "\n",
    "    x = w.value\n",
    "    x = np.divide(x, np.sum(x))\n",
    "\n",
    "    # Check Risk Parity Condition is actually met\n",
    "    risk_contrib = np.multiply(x, Q.dot(x))\n",
    "    if not np.all(np.isclose(risk_contrib, risk_contrib[0])):\n",
    "        print(\"RP did not work\")\n",
    "\n",
    "    return x\n",
    "\n",
    "'''\n",
    "Distributionally Robust Risk Parity With Wasserstein Distance Optimizer\n",
    "Inputs: mu: numpy array, key: Symbol. value: return estimate\n",
    "        Q: nxn Asset Covariance Matrix (n: # of assets)\n",
    "        delta: size of the uncertainty set\n",
    "Outputs: x: optimal allocations\n",
    "\n",
    "Formula:\n",
    "    \\min_{\\boldsymbol{\\phi} \\in \\mathcal{X}} {(\\sqrt{\\boldsymbol{\\phi}^T \\Sigma_{\\mathcal{P}}(R)\\boldsymbol{\\phi}} + \\sqrt{\\delta}||\\boldsymbol{\\phi}||_p)^2} - c\\sum_{i=1}^n ln(y)\n",
    "\n",
    "'''\n",
    "\n",
    "def DRRPW(mu,Q, delta=0):\n",
    "    \n",
    "    # # of Assets\n",
    "    n = len(mu)\n",
    "\n",
    "    # Decision Variables\n",
    "    w = cp.Variable(n)\n",
    "\n",
    "    # Kappa\n",
    "    k = 100\n",
    "\n",
    "    # Norm for x\n",
    "    p = 2\n",
    "\n",
    "    constraints = [\n",
    "        w>=0 # Disallow Short Sales\n",
    "    ]\n",
    "\n",
    "    # risk = cp.quad_form(w, Q)\n",
    "\n",
    "    log_term = 0\n",
    "    for i in range(n):\n",
    "        log_term += cp.log(w[i])\n",
    "    \n",
    "    # We need to compute \\sqrt{x^T Q x} intelligently because\n",
    "    # cvxpy does not compute well with the \\sqrt\n",
    "\n",
    "    # To do this, I will take the Cholesky decomposition\n",
    "    # Q = LL^T\n",
    "    # Then, take the 2-norm of L*x\n",
    "\n",
    "    # Idea: (L_1 * x_1)^2 = Q_1 x_1\n",
    "    \n",
    "    L = np.linalg.cholesky(Q)\n",
    "    L /= np.linalg.norm(L)\n",
    "    \n",
    "    obj = cp.power(cp.norm(L@w,2) + math.sqrt(delta)*cp.norm(w, p),2)\n",
    "    obj = obj - k*log_term\n",
    "\n",
    "    prob = cp.Problem(cp.Minimize(obj), constraints=constraints)\n",
    "    \n",
    "    # ECOS fails sometimes, if it does then do SCS\n",
    "    try:\n",
    "        prob.solve(verbose=False)\n",
    "    except:\n",
    "        prob.solve(solver='SCS',verbose=False)\n",
    "    \n",
    "    x = w.value\n",
    "    x = np.divide(x, np.sum(x))\n",
    "    \n",
    "    # Check Risk Parity Condition is actually met\n",
    "    # Note: DRRPW will not meet RP, will meet a robust version of RP\n",
    "    risk_contrib = np.multiply(x, Q.dot(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def drrpw_nominal_learnDelta(Q):\n",
    "    n_y = Q.shape[-1]\n",
    "    # Variables\n",
    "    phi = cp.Variable((n_y,1), nonneg=True)\n",
    "    t = cp.Variable()\n",
    "    \n",
    "    # Size of uncertainty set\n",
    "    delta = cp.Parameter(nonneg=True)\n",
    "    # T = cp.Parameter((n_y, n_y), PSD=True)\n",
    "\n",
    "    # Norm for x. TODO set this to be the Mahalanobis Norm\n",
    "    p = 2\n",
    "\n",
    "    # Kappa, dont need this to be trainable as the value of this doesnt really matter\n",
    "    k = 2\n",
    "\n",
    "    # We need to compute \\sqrt{x^T Q x} intelligently because\n",
    "    # cvxpy does not compute well with the \\sqrt\n",
    "\n",
    "    # To do this, I will take the Cholesky decomposition\n",
    "    # Q = LL^T\n",
    "    # Then, take the 2-norm of L*x\n",
    "\n",
    "    # Idea: (L_1 * x_1)^2 = Q_1 x_1\n",
    "\n",
    "    try:\n",
    "        L = np.linalg.cholesky(Q)\n",
    "    except:\n",
    "        Q = nearestPD(Q)\n",
    "        L = np.linalg.cholesky(Q)\n",
    "\n",
    "    L /= np.linalg.norm(L)\n",
    "\n",
    "    # Constraints\n",
    "    constraints = [\n",
    "        phi >= 0,\n",
    "        t >= cp.power(cp.norm(L@phi, 2) + delta*cp.norm(phi, p),2)\n",
    "        # t >= cp.power(cp.norm(L@phi, 2) + cp.norm(T@phi, 2),2)\n",
    "    ]\n",
    "\n",
    "    log_term = 0\n",
    "    for i in range(n_y):\n",
    "        log_term += cp.log(phi[i])\n",
    "\n",
    "    # obj = cp.power(cp.norm(L@w, 2) + delta*cp.norm(w, p),2)\n",
    "    # obj = cp.sum_squares(cp.norm(L@w, 2) + delta*cp.norm(w, p))\n",
    "    # cp.quad_form(w, Q)\n",
    "    # obj = cp.quad_form(w, Q) + 2*delta*cp.norm(w,2)*cp.norm(L@w, 2) + cp.norm(w,2)\n",
    "    # print('using this one')\n",
    "    # obj = 2*delta*cp.norm(w,2)*cp.norm(L@w_tilde, 2)\n",
    "    obj = (t) - k*log_term\n",
    "\n",
    "    # Objective function\n",
    "    objective = cp.Minimize(obj)    \n",
    "\n",
    "    # Construct optimization problem and differentiable layer\n",
    "    problem = cp.Problem(objective, constraints=constraints)\n",
    "\n",
    "    return CvxpyLayer(problem, parameters=[delta], variables=[phi, t])\n",
    "\n",
    "\n",
    "def drrpw_nominal_learnDelta_batched(n_y):\n",
    "    # Variables\n",
    "    phi = cp.Variable((n_y,1), nonneg=True)\n",
    "    t = cp.Variable()\n",
    "    \n",
    "    # Size of uncertainty set\n",
    "    delta = cp.Parameter(1, nonneg=True)\n",
    "\n",
    "    # L - Square Root of Covariance Matrix\n",
    "    L = cp.Parameter((n_y, n_y))\n",
    "\n",
    "    # Norm for x\n",
    "    p = 2\n",
    "\n",
    "    # Kappa, dont need this to be trainable as the value of this doesnt really matter\n",
    "    k = 2\n",
    "\n",
    "    # Constraints\n",
    "    constraints = [\n",
    "        phi >= 0,\n",
    "        t >= cp.power(cp.norm(L@phi, 2) + delta*cp.norm(phi, p),2)\n",
    "        # t >= cp.power(cp.norm(L@phi, 2) + cp.norm(T@phi, 2),2)\n",
    "    ]\n",
    "\n",
    "    log_term = 0\n",
    "    for i in range(n_y):\n",
    "        log_term += cp.log(phi[i])\n",
    "\n",
    "    obj = (t) - k*log_term\n",
    "\n",
    "    # Objective function\n",
    "    objective = cp.Minimize(obj)    \n",
    "\n",
    "    # Construct optimization problem and differentiable layer\n",
    "    problem = cp.Problem(objective, constraints=constraints)\n",
    "\n",
    "    return CvxpyLayer(problem, parameters=[delta, L], variables=[phi, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass: tensor([[[4.3807e-09],\n",
      "         [1.0000e+00],\n",
      "         [1.8199e-06]]])\n",
      "RP: [0.22014154 0.64599017 0.13386829]\n",
      "DRRPW: [0.03367387 0.94202728 0.02429885]\n"
     ]
    }
   ],
   "source": [
    "indx = 0\n",
    "layer = drrpw_nominal_learnDelta(Q[indx])\n",
    "delta = torch.zeros(1)\n",
    "z_star, _ = layer(delta)\n",
    "# Make each portfolio sum to 1\n",
    "sums = z_star.sum(dim=1, keepdim=True)\n",
    "z_stars = z_star / sums\n",
    "print('Forward Pass: {}'.format(z_stars))\n",
    "print('RP: {}'.format(RP([0]*3, Q[indx])))\n",
    "print('DRRPW: {}'.format(DRRPW([0]*3, Q[indx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09873581, 0.78478117, 0.11648302])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DRRPW(None, Q, delta=0.0016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_covariance_matrix(sliding_windows):\n",
    "    # get the number of batches and number of assets\n",
    "    num_batches, window_size, num_assets = sliding_windows.size()\n",
    "\n",
    "    # initialize covariance matrix tensor\n",
    "    covariance_matrix_tensor = torch.zeros((num_batches, num_assets, num_assets))\n",
    "\n",
    "    # compute covariance matrix for each batch\n",
    "    for i in range(num_batches):\n",
    "        batch = sliding_windows[i]\n",
    "        covariance_matrix = torch.Tensor(np.cov(batch.numpy().T))\n",
    "        covariance_matrix_tensor[i] = covariance_matrix\n",
    "\n",
    "    return covariance_matrix_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "compute_covariance_matrix(sliding_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting icsNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading ics-0.7.2-py2.py3-none-any.whl (40 kB)\n",
      "     ---------------------------------------- 40.1/40.1 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\rafay\\anaconda3\\lib\\site-packages (from ics) (21.4.0)\n",
      "Collecting tatsu>4.2\n",
      "  Downloading TatSu-5.8.3-py2.py3-none-any.whl (101 kB)\n",
      "     -------------------------------------- 101.5/101.5 kB 5.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>1.5 in c:\\users\\rafay\\anaconda3\\lib\\site-packages (from ics) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\rafay\\anaconda3\\lib\\site-packages (from ics) (2.8.2)\n",
      "Requirement already satisfied: arrow>=0.11 in c:\\users\\rafay\\anaconda3\\lib\\site-packages (from ics) (1.2.2)\n",
      "Installing collected packages: tatsu, ics\n",
      "Successfully installed ics-0.7.2 tatsu-5.8.3\n"
     ]
    }
   ],
   "source": [
    "%pip install ics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ics\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ics import Calendar, Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "c = Calendar()\n",
    "for week in range(15):\n",
    "    for day in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']:\n",
    "        date = date_list[i]\n",
    "        # print(plan.loc[plan['WEEK'] == float(week)])\n",
    "        do = plan.loc[plan['WEEK'] == (week+1)][day].values[0]\n",
    "        i+=1\n",
    "\n",
    "        e = Event(name=\"HM Training Day {}\".format(i), begin=date, description=do)\n",
    "        c.events.add(e)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my.ics', 'w') as my_file:\n",
    "    my_file.writelines(c.serialize_iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WEEK</th>\n",
       "      <th>Date</th>\n",
       "      <th>Monday</th>\n",
       "      <th>Tuesday</th>\n",
       "      <th>Wednesday</th>\n",
       "      <th>Thursday</th>\n",
       "      <th>Friday</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>Weekly Mileage</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>Walk / Run 10 x \\n2 min walk\\n1 min run</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>Walk / Run 10 x \\n2 min walk\\n1 min run</td>\n",
       "      <td>Strength Training 45-60 mins</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>Long Run 2.5k (walk if needed)</td>\n",
       "      <td>Around\\n9k</td>\n",
       "      <td>5K PHASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>Walk / Run 15 x \\n1 min walk\\n1 min run</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>Walk / Run 15 x \\n1 min walk\\n1 min run</td>\n",
       "      <td>Strength Training 45-60 mins</td>\n",
       "      <td>Walk / Run 15 x \\n1 min walk\\n1 min run</td>\n",
       "      <td>Long Run 3k (walk if needed)</td>\n",
       "      <td>Around\\n13k</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>Walk / Run 15 x \\n0.5 min walk\\n1.5 min run</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>Walk / Run 15 x \\n0.5 min walk\\n1.5 min run</td>\n",
       "      <td>Strength Training 45-60 mins</td>\n",
       "      <td>Walk / Run 15 x \\n0.5 min walk\\n1.5 min run</td>\n",
       "      <td>Long Run 4k (walk if needed)</td>\n",
       "      <td>Around\\n15k</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>Walk / Run 10 x \\n1 min walk\\n2 min run</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>Walk / Run 10 x \\n1 min walk\\n2 min run</td>\n",
       "      <td>Strength Training 45-60 mins</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>5 k</td>\n",
       "      <td>Around\\n13k</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>4 k</td>\n",
       "      <td>3 k</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>4 k</td>\n",
       "      <td>Strength Training</td>\n",
       "      <td>6 k</td>\n",
       "      <td>17 k</td>\n",
       "      <td>10K PHASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>4 k</td>\n",
       "      <td>3 k</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>4 k</td>\n",
       "      <td>Strength Training</td>\n",
       "      <td>7 k</td>\n",
       "      <td>18 k</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>5 k</td>\n",
       "      <td>7 k</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>5 k</td>\n",
       "      <td>Strength Training</td>\n",
       "      <td>8 k</td>\n",
       "      <td>25 k</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>5 k</td>\n",
       "      <td>7 k</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>5 k</td>\n",
       "      <td>Strength Training</td>\n",
       "      <td>10 k</td>\n",
       "      <td>27 k</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>5 k</td>\n",
       "      <td>7 k</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>5 k</td>\n",
       "      <td>Strength Training</td>\n",
       "      <td>8 k</td>\n",
       "      <td>25 k</td>\n",
       "      <td>HALF MARATHON  PHASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>5 k</td>\n",
       "      <td>7 k</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>5 k</td>\n",
       "      <td>Strength Training</td>\n",
       "      <td>11 k</td>\n",
       "      <td>28 k</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>7 k</td>\n",
       "      <td>8 k</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>7 k</td>\n",
       "      <td>Strength Training</td>\n",
       "      <td>12 k</td>\n",
       "      <td>34 k</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>7 k</td>\n",
       "      <td>8 k</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>7 k</td>\n",
       "      <td>Strength Training</td>\n",
       "      <td>10 k</td>\n",
       "      <td>32 k</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>7 k</td>\n",
       "      <td>8 k</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>7 k</td>\n",
       "      <td>Strength Training</td>\n",
       "      <td>15 k</td>\n",
       "      <td>37 k</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>7 k</td>\n",
       "      <td>10 k</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>7 k</td>\n",
       "      <td>Strength Training</td>\n",
       "      <td>17 k</td>\n",
       "      <td>41 k</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>5 k</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>5 k</td>\n",
       "      <td>Rest Day</td>\n",
       "      <td>3 k</td>\n",
       "      <td>21.1 k</td>\n",
       "      <td>34 k</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    WEEK  Date    Monday                                      Tuesday  \\\n",
       "0    1.0   NaN  Rest Day      Walk / Run 10 x \\n2 min walk\\n1 min run   \n",
       "3    2.0   NaN  Rest Day      Walk / Run 15 x \\n1 min walk\\n1 min run   \n",
       "6    3.0   NaN  Rest Day  Walk / Run 15 x \\n0.5 min walk\\n1.5 min run   \n",
       "9    4.0   NaN  Rest Day      Walk / Run 10 x \\n1 min walk\\n2 min run   \n",
       "12   5.0   NaN  Rest Day                                          4 k   \n",
       "15   6.0   NaN  Rest Day                                          4 k   \n",
       "18   7.0   NaN  Rest Day                                          5 k   \n",
       "21   8.0   NaN  Rest Day                                          5 k   \n",
       "24   9.0   NaN  Rest Day                                          5 k   \n",
       "27  10.0   NaN  Rest Day                                          5 k   \n",
       "30  11.0   NaN  Rest Day                                          7 k   \n",
       "33  12.0   NaN  Rest Day                                          7 k   \n",
       "36  13.0   NaN  Rest Day                                          7 k   \n",
       "39  14.0   NaN  Rest Day                                          7 k   \n",
       "42  15.0   NaN  Rest Day                                          5 k   \n",
       "\n",
       "   Wednesday                                     Thursday  \\\n",
       "0   Rest Day      Walk / Run 10 x \\n2 min walk\\n1 min run   \n",
       "3   Rest Day      Walk / Run 15 x \\n1 min walk\\n1 min run   \n",
       "6   Rest Day  Walk / Run 15 x \\n0.5 min walk\\n1.5 min run   \n",
       "9   Rest Day      Walk / Run 10 x \\n1 min walk\\n2 min run   \n",
       "12       3 k                                     Rest Day   \n",
       "15       3 k                                     Rest Day   \n",
       "18       7 k                                     Rest Day   \n",
       "21       7 k                                     Rest Day   \n",
       "24       7 k                                     Rest Day   \n",
       "27       7 k                                     Rest Day   \n",
       "30       8 k                                     Rest Day   \n",
       "33       8 k                                     Rest Day   \n",
       "36       8 k                                     Rest Day   \n",
       "39      10 k                                     Rest Day   \n",
       "42  Rest Day                                          5 k   \n",
       "\n",
       "                          Friday                                     Saturday  \\\n",
       "0   Strength Training 45-60 mins                                     Rest Day   \n",
       "3   Strength Training 45-60 mins      Walk / Run 15 x \\n1 min walk\\n1 min run   \n",
       "6   Strength Training 45-60 mins  Walk / Run 15 x \\n0.5 min walk\\n1.5 min run   \n",
       "9   Strength Training 45-60 mins                                     Rest Day   \n",
       "12                           4 k                            Strength Training   \n",
       "15                           4 k                            Strength Training   \n",
       "18                           5 k                            Strength Training   \n",
       "21                           5 k                            Strength Training   \n",
       "24                           5 k                            Strength Training   \n",
       "27                           5 k                            Strength Training   \n",
       "30                           7 k                            Strength Training   \n",
       "33                           7 k                            Strength Training   \n",
       "36                           7 k                            Strength Training   \n",
       "39                           7 k                            Strength Training   \n",
       "42                      Rest Day                                          3 k   \n",
       "\n",
       "                            Sunday Weekly Mileage           Unnamed: 10  \n",
       "0   Long Run 2.5k (walk if needed)     Around\\n9k              5K PHASE  \n",
       "3     Long Run 3k (walk if needed)    Around\\n13k                   NaN  \n",
       "6     Long Run 4k (walk if needed)    Around\\n15k                   NaN  \n",
       "9                              5 k    Around\\n13k                   NaN  \n",
       "12                             6 k           17 k             10K PHASE  \n",
       "15                             7 k           18 k                   NaN  \n",
       "18                             8 k           25 k                   NaN  \n",
       "21                            10 k           27 k                   NaN  \n",
       "24                             8 k           25 k  HALF MARATHON  PHASE  \n",
       "27                            11 k           28 k                   NaN  \n",
       "30                            12 k           34 k                   NaN  \n",
       "33                            10 k           32 k                   NaN  \n",
       "36                            15 k           37 k                   NaN  \n",
       "39                            17 k           41 k                   NaN  \n",
       "42                          21.1 k           34 k                   NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan = plan.dropna(subset='WEEK')\n",
    "plan.head(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (i) Ledoit-Wolfe Shrinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrinkage(returns):\n",
    "    \"\"\"Shrinks sample covariance matrix towards constant correlation unequal variance matrix.\n",
    "    Ledoit & Wolf (\"Honey, I shrunk the sample covariance matrix\", Portfolio Management, 30(2004),\n",
    "    110-119) optimal asymptotic shrinkage between 0 (sample covariance matrix) and 1 (constant\n",
    "    sample average correlation unequal sample variance matrix).\n",
    "    Paper:\n",
    "    http://www.ledoit.net/honey.pdf\n",
    "    Matlab code:\n",
    "    https://www.econ.uzh.ch/dam/jcr:ffffffff-935a-b0d6-ffff-ffffde5e2d4e/covCor.m.zip\n",
    "    Special thanks to Evgeny Pogrebnyak https://github.com/epogrebnyak\n",
    "    :param returns:\n",
    "        t, n - returns of t observations of n shares.\n",
    "    :return:\n",
    "        Covariance matrix, sample average correlation, shrinkage.\n",
    "    \"\"\"\n",
    "    t, n = returns.shape\n",
    "    mean_returns = np.mean(returns, axis=0, keepdims=True)\n",
    "    returns -= mean_returns\n",
    "    sample_cov = returns.transpose() @ returns / t\n",
    "\n",
    "    # sample average correlation\n",
    "    var = np.diag(sample_cov).reshape(-1, 1)\n",
    "    sqrt_var = var ** 0.5\n",
    "    unit_cor_var = sqrt_var * sqrt_var.transpose()\n",
    "    average_cor = ((sample_cov / unit_cor_var).sum() - n) / n / (n - 1)\n",
    "    prior = average_cor * unit_cor_var\n",
    "    np.fill_diagonal(prior, var)\n",
    "\n",
    "    # pi-hat\n",
    "    y = returns ** 2\n",
    "    phi_mat = (y.transpose() @ y) / t - sample_cov ** 2\n",
    "    phi = phi_mat.sum()\n",
    "\n",
    "    # rho-hat\n",
    "    theta_mat = ((returns ** 3).transpose() @ returns) / t - var * sample_cov\n",
    "    np.fill_diagonal(theta_mat, 0)\n",
    "    rho = (\n",
    "        np.diag(phi_mat).sum()\n",
    "        + average_cor * (1 / sqrt_var @ sqrt_var.transpose() * theta_mat).sum()\n",
    "    )\n",
    "\n",
    "    # gamma-hat\n",
    "    gamma = np.linalg.norm(sample_cov - prior, \"fro\") ** 2\n",
    "\n",
    "    # shrinkage constant\n",
    "    kappa = (phi - rho) / gamma\n",
    "    shrink = max(0, min(1, kappa / t))\n",
    "\n",
    "    # estimator\n",
    "    sigma = shrink * prior + (1 - shrink) * sample_cov\n",
    "\n",
    "    return sigma, average_cor, shrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma, average_cor, shrink = shrinkage(returns.drop('date', axis=1).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.11719612e-04, 1.05466722e-05, 1.72681015e-04],\n",
       "       [1.05466722e-05, 2.97840162e-05, 3.20162546e-05],\n",
       "       [1.72681015e-04, 3.20162546e-05, 5.96583123e-04]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
